\begin{thebibliography}{Hub21b}

\bibitem[AS]{d2l:21}
{\sc Amazon} \btxandlong{}\ {\sc Google Scientists}: {\em Dive into Deep
  Learning}.

\bibitem[Hub21a]{BERTMM:21}
{\sc Hub, TensorFlow}: {\em BERT multi cased Model: L=12, H=768, A=12}, 2021.

\bibitem[Hub21b]{tfhub:21}
{\sc Hub, TensorFlow}: {\em Tensorflow Hub Modelle}, 2021.

\bibitem[JDT19]{BERT:19}
{\sc Jacob~Devlin, Ming-Wei~Chang, Kenton~Lee} \btxandlong{}\ {\sc Kristina
  Toutanova}: {\em BERT: Pre-training of Deep Bidirectional Transformers for
  Language Understanding}.
\newblock Google Publication, 2019.

\bibitem[Mer]{wikitext2:20}
{\sc Merity, Stephen}: {\em The WikiText Long Term Dependency Language Modeling
  Dataset}.

\bibitem[Rot00]{denis_Transformer:02}
{\sc Rothman, Denis}: {\em Transformer for Natural Language Processing}.
\newblock TODO:, 1000.

\bibitem[Vas17]{Vaswani:2017}
{\sc Vaswanni, Ashish}: {\em Attention Is All You Need}.
\newblock Google Publication, 2017.

\bibitem[Web21]{transformer_tensorflow:21}
{\sc Website, TensorFlow}: {\em Transformer model for language understanding},
  2021.

\bibitem[Wie21]{CO:19}
{\sc Wierenga, Rick}: {\em An Empirical Comparison of Optimizers for Machine
  Learning Models}, 2021.

\end{thebibliography}
